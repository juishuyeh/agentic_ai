# langgraph_use_mcp_as_server.py
from datetime import datetime
from typing import List, TypedDict

from dotenv import load_dotenv
from fastmcp import Context, FastMCP
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph

load_dotenv()
# å®šç¾©ç‹€æ…‹é¡å‹
class TextProcessState(TypedDict):
    input_text: str
    processed_text: str
    ai_response: str
    steps: List[str]

# åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model(
    model="openai:openai/gpt-oss-20b-local",
    temperature=0.7
)

# å»ºç«‹ FastMCP å¯¦ä¾‹
mcp = FastMCP("Simple-FastMCP-LangGraph")

def create_text_processing_graph():
    """å»ºç«‹æ–‡å­—è™•ç†çš„ LangGraph å·¥ä½œæµ"""
    
    async def preprocess_text(state: TextProcessState) -> TextProcessState:
        """é è™•ç†æ–‡å­—"""
        processed = state["input_text"].strip()
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            **state,
            "processed_text": processed,
            "steps": state["steps"] + [f"æ–‡å­—é è™•ç†å®Œæˆ ({timestamp})"]
        }
    
    async def generate_ai_response(state: TextProcessState) -> TextProcessState:
        """ç”Ÿæˆ AI å›æ‡‰"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡å­—åˆ†æåŠ©æ‰‹ã€‚"),
            ("human", "è«‹åˆ†æä»¥ä¸‹æ–‡å­—ï¼š\n\n{text}")
        ])
        
        response = await model.ainvoke(
            prompt.format_messages(text=state["processed_text"])
        )
        
        return {
            **state,
            "ai_response": response.content,
            "steps": state["steps"] + ["AI åˆ†æå®Œæˆ"]
        }
    
    # å»ºç«‹å·¥ä½œæµåœ–
    workflow = StateGraph(TextProcessState)
    workflow.add_node("preprocess", preprocess_text)
    workflow.add_node("ai_analyze", generate_ai_response)
    
    workflow.add_edge(START, "preprocess")
    workflow.add_edge("preprocess", "ai_analyze")
    workflow.add_edge("ai_analyze", END)
    
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# å»ºç«‹å…¨åŸŸ LangGraph å¯¦ä¾‹
text_processor = create_text_processing_graph()

# è¨»å†Šç‚º MCP å·¥å…·
@mcp.tool()
async def process_text_with_langgraph(text: str, ctx: Context = None) -> str:
    """
    ä½¿ç”¨ LangGraph è™•ç†æ–‡å­—
    
    Args:
        text: è¦è™•ç†çš„æ–‡å­—å…§å®¹
    
    Returns:
        è™•ç†çµæœ
    """
    if ctx:
        await ctx.info(f"é–‹å§‹åˆ†ææ–‡å­—: {text[:30]}...")
    
    # åˆå§‹ç‹€æ…‹
    initial_state = {
        "input_text": text,
        "processed_text": "",
        "ai_response": "",
        "steps": []
    }
    
    # åŸ·è¡Œå·¥ä½œæµ
    final_state = await text_processor.ainvoke(
        initial_state,
        config={"configurable": {"thread_id": f"analyze_{datetime.now()}"}}
    )
    
    # æ ¼å¼åŒ–çµæœ
    result = f"""ğŸ“Š æ–‡å­—åˆ†æçµæœ

ğŸ“ åŸå§‹æ–‡å­—:
{final_state['input_text']}

ğŸ¤– AI åˆ†æ:
{final_state['ai_response']}

âš™ï¸ è™•ç†æ­¥é©Ÿ:
{' â†’ '.join(final_state['steps'])}"""
    
    return result

if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹• Simple FastMCP + LangGraph ä¼ºæœå™¨")
    print("ğŸŒ ä¼ºæœå™¨åœ°å€: http://127.0.0.1:8004/mcp")
    
    mcp.run(
        transport="http",
        host="127.0.0.1",
        port=8004,
        log_level="info"
    )